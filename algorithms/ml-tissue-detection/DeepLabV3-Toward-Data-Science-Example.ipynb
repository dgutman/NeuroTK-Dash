{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e9267f-23a6-475e-8bbf-df404d196929",
   "metadata": {},
   "source": [
    "# DeepLabV3: Toward Data Science Example\n",
    "Source: https://towardsdatascience.com/transfer-learning-for-segmentation-using-deeplabv3-in-pytorch-f770863d6a42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc73526-d425-4a50-8b43-85346519c69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33827217-f1f0-4f66-a3a2-879bd0ae4440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('/jcDataStore/Projects/NeuroTK-Dash')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Optional\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import time\n",
    "import csv\n",
    "import copy\n",
    "\n",
    "import os\n",
    "from os.path import join, isfile\n",
    "\n",
    "from neurotk import imread, imwrite\n",
    "from neurotk.utils import create_dirs\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision.models.segmentation.deeplabv3 import (\n",
    "    DeepLabHead, DeepLabV3_ResNet101_Weights\n",
    ")\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a669b575-fbcb-4999-b1f0-fd1469de1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Manpreet Singh Minhas\n",
    "Contact: msminhas at uwaterloo ca\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "\n",
    "\n",
    "class SegmentationDataset(VisionDataset):\n",
    "    \"\"\"A PyTorch dataset for image segmentation task.\n",
    "    The dataset is compatible with torchvision transforms.\n",
    "    The transforms passed would be applied to both the Images and Masks.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 image_folder: str,\n",
    "                 mask_folder: str,\n",
    "                 transforms: Optional[Callable] = None,\n",
    "                 seed: int = None,\n",
    "                 fraction: float = None,\n",
    "                 subset: str = None,\n",
    "                 image_color_mode: str = \"rgb\",\n",
    "                 mask_color_mode: str = \"grayscale\") -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory path.\n",
    "            image_folder (str): Name of the folder that contains the images in the root directory.\n",
    "            mask_folder (str): Name of the folder that contains the masks in the root directory.\n",
    "            transforms (Optional[Callable], optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.ToTensor`` for images. Defaults to None.\n",
    "            seed (int, optional): Specify a seed for the train and test split for reproducible results. Defaults to None.\n",
    "            fraction (float, optional): A float value from 0 to 1 which specifies the validation split fraction. Defaults to None.\n",
    "            subset (str, optional): 'Train' or 'Test' to select the appropriate set. Defaults to None.\n",
    "            image_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'rgb'.\n",
    "            mask_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'grayscale'.\n",
    "\n",
    "        Raises:\n",
    "            OSError: If image folder doesn't exist in root.\n",
    "            OSError: If mask folder doesn't exist in root.\n",
    "            ValueError: If subset is not either 'Train' or 'Test'\n",
    "            ValueError: If image_color_mode and mask_color_mode are either 'rgb' or 'grayscale'\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms)\n",
    "        image_folder_path = Path(self.root) / image_folder\n",
    "        mask_folder_path = Path(self.root) / mask_folder\n",
    "        if not image_folder_path.exists():\n",
    "            raise OSError(f\"{image_folder_path} does not exist.\")\n",
    "        if not mask_folder_path.exists():\n",
    "            raise OSError(f\"{mask_folder_path} does not exist.\")\n",
    "\n",
    "        if image_color_mode not in [\"rgb\", \"grayscale\"]:\n",
    "            raise ValueError(\n",
    "                f\"{image_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
    "            )\n",
    "        if mask_color_mode not in [\"rgb\", \"grayscale\"]:\n",
    "            raise ValueError(\n",
    "                f\"{mask_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
    "            )\n",
    "\n",
    "        self.image_color_mode = image_color_mode\n",
    "        self.mask_color_mode = mask_color_mode\n",
    "\n",
    "        if not fraction:\n",
    "            self.image_names = sorted(image_folder_path.glob(\"*\"))\n",
    "            self.mask_names = sorted(mask_folder_path.glob(\"*\"))\n",
    "        else:\n",
    "            if subset not in [\"Train\", \"Test\"]:\n",
    "                raise (ValueError(\n",
    "                    f\"{subset} is not a valid input. Acceptable values are Train and Test.\"\n",
    "                ))\n",
    "            self.fraction = fraction\n",
    "            self.image_list = np.array(sorted(image_folder_path.glob(\"*\")))\n",
    "            self.mask_list = np.array(sorted(mask_folder_path.glob(\"*\")))\n",
    "            if seed:\n",
    "                np.random.seed(seed)\n",
    "                indices = np.arange(len(self.image_list))\n",
    "                np.random.shuffle(indices)\n",
    "                self.image_list = self.image_list[indices]\n",
    "                self.mask_list = self.mask_list[indices]\n",
    "            if subset == \"Train\":\n",
    "                self.image_names = self.image_list[:int(\n",
    "                    np.ceil(len(self.image_list) * (1 - self.fraction)))]\n",
    "                self.mask_names = self.mask_list[:int(\n",
    "                    np.ceil(len(self.mask_list) * (1 - self.fraction)))]\n",
    "            else:\n",
    "                self.image_names = self.image_list[\n",
    "                    int(np.ceil(len(self.image_list) * (1 - self.fraction))):]\n",
    "                self.mask_names = self.mask_list[\n",
    "                    int(np.ceil(len(self.mask_list) * (1 - self.fraction))):]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        image_path = self.image_names[index]\n",
    "        mask_path = self.mask_names[index]\n",
    "        with open(image_path, \"rb\") as image_file, open(mask_path,\n",
    "                                                        \"rb\") as mask_file:\n",
    "            image = Image.open(image_file)\n",
    "            if self.image_color_mode == \"rgb\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            elif self.image_color_mode == \"grayscale\":\n",
    "                image = image.convert(\"L\")\n",
    "            mask = Image.open(mask_file)\n",
    "            if self.mask_color_mode == \"rgb\":\n",
    "                mask = mask.convert(\"RGB\")\n",
    "            elif self.mask_color_mode == \"grayscale\":\n",
    "                mask = mask.convert(\"L\")\n",
    "            sample = {\"image\": image, \"mask\": mask}\n",
    "            if self.transforms:\n",
    "                sample[\"image\"] = self.transforms(sample[\"image\"])\n",
    "                sample[\"mask\"] = self.transforms(sample[\"mask\"])\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfca1ed-2c25-429f-bf38-585680eb8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to create dataloaders.\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def get_dataloader_single_folder(data_dir: str,\n",
    "                                 image_folder: str = 'Images',\n",
    "                                 mask_folder: str = 'Masks',\n",
    "                                 fraction: float = 0.2,\n",
    "                                 batch_size: int = 4):\n",
    "    \"\"\"Create train and test dataloader from a single directory containing\n",
    "    the image and mask folders.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Data directory path or root\n",
    "        image_folder (str, optional): Image folder name. Defaults to 'Images'.\n",
    "        mask_folder (str, optional): Mask folder name. Defaults to 'Masks'.\n",
    "        fraction (float, optional): Fraction of Test set. Defaults to 0.2.\n",
    "        batch_size (int, optional): Dataloader batch size. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        dataloaders: Returns dataloaders dictionary containing the\n",
    "        Train and Test dataloaders.\n",
    "    \"\"\"\n",
    "    data_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    image_datasets = {\n",
    "        x: SegmentationDataset(data_dir,\n",
    "                               image_folder=image_folder,\n",
    "                               mask_folder=mask_folder,\n",
    "                               seed=100,\n",
    "                               fraction=fraction,\n",
    "                               subset=x,\n",
    "                               transforms=data_transforms)\n",
    "        for x in ['Train', 'Test']\n",
    "    }\n",
    "    dataloaders = {\n",
    "        x: DataLoader(image_datasets[x],\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=8)\n",
    "        for x in ['Train', 'Test']\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb253af-f210-421b-9e3b-f594b26b7f84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained DeepLabV3 model with desired number of outputs.\n",
    "def createDeepLabv3(outputchannels: bool = 1):\n",
    "    \"\"\"DeepLabv3 class with custom head\n",
    "    Args:\n",
    "        outputchannels (int, optional): The number of output channels\n",
    "        in your dataset masks. Defaults to 1.\n",
    "    Returns:\n",
    "        model: Returns the DeepLabv3 model with the ResNet101 backbone.\n",
    "    \"\"\"\n",
    "    model = models.segmentation.deeplabv3_resnet101(\n",
    "        weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1,\n",
    "        progress=False,\n",
    "    )\n",
    "    \n",
    "    model.classifier = DeepLabHead(2048, outputchannels)\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcf839-91c3-4baf-a292-f2f3c540e02e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training function.\n",
    "def train_model(model, criterion, dataloaders, optimizer, metrics, bpath,\n",
    "                num_epochs):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    # Use gpu if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Initialize the log file for training and testing loss and metrics\n",
    "    fieldnames = ['epoch', 'Train_loss', 'Test_loss'] + \\\n",
    "        [f'Train_{m}' for m in metrics.keys()] + \\\n",
    "        [f'Test_{m}' for m in metrics.keys()]\n",
    "    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        # Initialize batch summary\n",
    "        batchsummary = {a: [0] for a in fieldnames}\n",
    "\n",
    "        for phase in ['Train', 'Test']:\n",
    "            if phase == 'Train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over data.\n",
    "            for sample in tqdm(iter(dataloaders[phase])):\n",
    "                inputs = sample['image'].to(device)\n",
    "                masks = sample['mask'].to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'Train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs['out'], masks)\n",
    "                    y_pred = outputs['out'].data.cpu().numpy().ravel()\n",
    "                    y_true = masks.data.cpu().numpy().ravel()\n",
    "                    for name, metric in metrics.items():\n",
    "                        if name == 'f1_score':\n",
    "                            # Use a classification threshold of 0.1\n",
    "                            batchsummary[f'{phase}_{name}'].append(\n",
    "                                metric(y_true > 0, y_pred > 0.1))\n",
    "                        else:\n",
    "                            batchsummary[f'{phase}_{name}'].append(\n",
    "                                metric(y_true.astype('uint8'), y_pred))\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'Train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            batchsummary['epoch'] = epoch\n",
    "            epoch_loss = loss\n",
    "            batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
    "            print('{} Loss: {:.4f}'.format(phase, loss))\n",
    "        for field in fieldnames[3:]:\n",
    "            batchsummary[field] = np.mean(batchsummary[field])\n",
    "        print(batchsummary)\n",
    "        with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow(batchsummary)\n",
    "            # deep copy the model\n",
    "            if phase == 'Test' and loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceecaef-26b1-4cb8-a2f7-7f5e0068c977",
   "metadata": {},
   "source": [
    "## Create dataset to test example code.\n",
    "There is not a lot of information of what the dataset should look like, but ideally it should be images and binary masks with positive class as either 1 or 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf10c5eb-399f-4ae1-b7e1-9ad046c09b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory of images and masks, put all on one.\n",
    "data_dir = '/jcDataStore/Data/NeuroTK-Dash/ml-tissue-detection/DeepLabV3'\n",
    "\n",
    "img_dir = join(data_dir, 'images')\n",
    "mask_dir = join(data_dir, 'masks')\n",
    "\n",
    "create_dirs([img_dir, mask_dir])\n",
    "\n",
    "# Directory with original low res. WSIs and tissue masks.\n",
    "# src_dir = '/jcDataStore/Data/nft-ai-project/wsi-inference/tissue-masks'\n",
    "src_dir = '/jcDataStore/Data/NeuroTK-Dash/ml-tissue-detection/gray-matter-dataset'\n",
    "\n",
    "for img_fp in tqdm(glob(join(src_dir, 'images', '*.png'))):\n",
    "    # Skip if the image & mask have already been saved.\n",
    "    fn = os.path.basename(img_fp)\n",
    "    img_save_fp = join(img_dir, fn)\n",
    "    mask_save_fp = join(mask_dir, fn)\n",
    "\n",
    "    if  isfile(img_save_fp) and isfile(mask_save_fp):\n",
    "        continue\n",
    "        \n",
    "    # Read the image and resize to standard size.\n",
    "    img = imread(img_fp)\n",
    "    img = cv.resize(img, (256, 256))\n",
    "\n",
    "    # Repeat with the mask.\n",
    "    mask = imread(join(src_dir, 'masks', fn))\n",
    "    mask = mask[:, :, 0]  # Grab only one channel\n",
    "    mask = cv.resize(mask, (256, 256))\n",
    "\n",
    "    # Save in new locations.\n",
    "    imwrite(img_save_fp, img)\n",
    "    imwrite(mask_save_fp, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af08c2e-f5c0-48af-ae36-c86c51f6e3a2",
   "metadata": {},
   "source": [
    "## Test out Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a845cda1-e1c5-4565-a60d-f3591d9f0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model.\n",
    "model = createDeepLabv3()\n",
    "_ = model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e19645c-d69f-450d-9baa-75ab99a11390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create location to save training results.\n",
    "exp_dir = Path(\n",
    "    '/jcDataStore/Data/NeuroTK-Dash/ml-tissue-detection/DeepLabV3/exps'\n",
    ")\n",
    "exp_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70084358-eaa8-4bd7-b0a6-da68aeb3f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the loss function\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Specify the optimizer with a lower learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Specify the evaluation metrics\n",
    "metrics = {'f1_score': f1_score, 'auroc': roc_auc_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f508aa-48c3-4500-9ed8-1aff650c0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders.\n",
    "data_directory = Path('/jcDataStore/Data/NeuroTK-Dash/ml-tissue-detection/gray-matter-dataset')\n",
    "\n",
    "dataloaders = get_dataloader_single_folder(\n",
    "    data_directory, batch_size=8, image_folder = 'images', \n",
    "    mask_folder='masks'\n",
    ")\n",
    "\n",
    "_ = train_model(\n",
    "    model,\n",
    "    criterion,\n",
    "    dataloaders,\n",
    "    optimizer,\n",
    "    bpath=exp_dir,\n",
    "    metrics=metrics,\n",
    "    num_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313498b-56ce-467a-a131-6647979d4be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample image.\n",
    "model.eval()\n",
    "\n",
    "img = Image.open('/jcDataStore/Data/NeuroTK-Dash/ml-tissue-detection/gray-matter-dataset/images/A21-17_1_BIELS.png')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "for batch in dataloaders['Test']:\n",
    "    img = batch['image'].to('cpu')\n",
    "    mask = batch['mask'].to('cpu')\n",
    "\n",
    "    # predict\n",
    "    preds = model(img)['out']\n",
    "\n",
    "    # Plot each\n",
    "    for i in range(8):\n",
    "        image = img[i, :, :, :].detach().numpy()\n",
    "        image = np.moveaxis(image, 0, 2)\n",
    "        m = mask[i, 0, :, :].detach().numpy()\n",
    "        pred = preds[i, 0, :, :].detach().numpy() > 0.5\n",
    "\n",
    "        fig = plt.figure(figsize=(7, 4))\n",
    "        fig.add_subplot(1, 3, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        fig.add_subplot(1, 3, 2)\n",
    "        plt.imshow(m)\n",
    "        plt.title('Ground Truth Mask', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        fig.add_subplot(1, 3, 3)\n",
    "        plt.imshow(pred)\n",
    "        plt.axis('off')\n",
    "        plt.title('Prediction', fontsize=12)\n",
    "        plt.show()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-tissue-detection]",
   "language": "python",
   "name": "conda-env-ml-tissue-detection-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
